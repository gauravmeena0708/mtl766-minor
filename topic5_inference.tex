\subsection*{Question 1}
\textbf{Question:} For a random sample $\mathbf{X}_1, \dots, \mathbf{X}_n$ from a population with probability density function (pdf) $f(\mathbf{x} | \boldsymbol{\theta})$, write down the likelihood function $L(\boldsymbol{\theta})$.

\textbf{Solution:}
The likelihood function is the joint pdf of the observed data, viewed as a function of the parameters $\boldsymbol{\theta}$. Assuming the observations are independent and identically distributed, the likelihood function is:
$$ L(\boldsymbol{\theta} | \mathbf{x}_1, \dots, \mathbf{x}_n) = \prod_{i=1}^n f(\mathbf{x}_i | \boldsymbol{\theta}) $$

\subsection*{Question 2}
\textbf{Question:} What is the principle of maximum likelihood estimation (MLE)?

\textbf{Solution:}
The principle of maximum likelihood estimation is to find the value of the parameter vector $\boldsymbol{\theta}$ that maximizes the likelihood function $L(\boldsymbol{\theta})$. This value, denoted $\hat{\boldsymbol{\theta}}$, is the one that makes the observed data most probable. In practice, it is often easier to maximize the log-likelihood function, $\ln L(\boldsymbol{\theta})$.

\subsection*{Question 3}
\textbf{Question:} For a random sample from $N_p(\boldsymbol{\mu}, \Sigma)$ with known $\Sigma$, derive the MLE for $\boldsymbol{\mu}$.

\textbf{Solution:}
The log-likelihood function (ignoring constants) is:
$$ \ln L(\boldsymbol{\mu}) = -\frac{1}{2} \sum_{i=1}^n (\mathbf{x}_i - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}_i - \boldsymbol{\mu}) $$
To find the maximum, we take the derivative with respect to $\boldsymbol{\mu}$ and set it to zero.
$$ \frac{\partial \ln L(\boldsymbol{\mu})}{\partial \boldsymbol{\mu}} = \sum_{i=1}^n \Sigma^{-1}(\mathbf{x}_i - \boldsymbol{\mu}) = \mathbf{0} \implies \hat{\boldsymbol{\mu}} = \bar{\mathbf{X}} $$

\subsection*{Question 4}
\textbf{Question:} For a random sample from $N_p(\boldsymbol{\mu}, \Sigma)$, the MLEs are $\hat{\boldsymbol{\mu}} = \bar{\mathbf{X}}$ and $\hat{\Sigma} = \frac{1}{n}\sum_{i=1}^n(\mathbf{X}_i-\bar{\mathbf{X}})(\mathbf{X}_i-\bar{\mathbf{X}})^T$. How does $\hat{\Sigma}$ relate to the sample covariance matrix $S$? Is $\hat{\Sigma}$ an unbiased estimator of $\Sigma$?

\textbf{Solution:}
The relationship is $\hat{\Sigma} = \frac{n-1}{n}S$. The MLE $\hat{\Sigma}$ is a biased estimator of $\Sigma$. The sample covariance matrix $S$ is the unbiased estimator.

\subsection*{Question 5}
\textbf{Question:} Define the Wishart distribution.

\textbf{Solution:}
The Wishart distribution, $W_p(n, \Sigma)$, is the distribution of the matrix $A = \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T$, where each $\mathbf{X}_i$ is an independent random vector from $N_p(\mathbf{0}, \Sigma)$. It is a multivariate generalization of the chi-square distribution.

\subsection*{Question 6}
\textbf{Question:} Let $\mathbf{A} \sim W_p(n, \boldsymbol{\Sigma})$. Prove that $\mathbb{E}(\mathbf{A}) = n\boldsymbol{\Sigma}$.

\textbf{Solution:}
$\mathbb{E}(\mathbf{A}) = \mathbb{E}\left[\sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T\right] = \sum_{i=1}^n \mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T]$. Since $\mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T] = \text{Cov}(\mathbf{X}_i) + \mathbb{E}[\mathbf{X}_i]\mathbb{E}[\mathbf{X}_i]^T = \boldsymbol{\Sigma} + \mathbf{0} = \boldsymbol{\Sigma}$, the result follows.

\subsection*{Question 7}
\textbf{Question:} Let $\mathbf{x}_1, \dots, \mathbf{x}_n$ be a random sample from $\mathcal{N}_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. Prove or disprove that the distribution of $n(\bar{\mathbf{X}} - \boldsymbol{\mu})^T \mathbf{S}^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu})$ converges to a chi-square distribution as $n \to \infty$.

\textbf{Solution:}
True. By the Law of Large Numbers, $\mathbf{S} \xrightarrow{p} \boldsymbol{\Sigma}$. By the multivariate CLT, $\sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu}) \xrightarrow{d} N_p(\mathbf{0}, \boldsymbol{\Sigma})$. By Slutsky's theorem, the statistic converges in distribution to $\mathbf{Y}^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}$ where $\mathbf{Y} \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$, which is the definition of a $\chi^2_p$ distribution.

\subsection*{Question 8}
\textbf{Question:} Let $\mathbf{X}_1, \dots, \mathbf{X}_n$ be a random sample from a population with mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$. Show that the asymptotic distribution of $n(\bar{\mathbf{X}} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu})$ is $\chi^2_p$.

\textbf{Solution:}
By the multivariate CLT, $\mathbf{Y}_n = \sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu}) \xrightarrow{d} \mathbf{Y} \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$. By the continuous mapping theorem, the distribution of $\mathbf{Y}_n^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}_n$ converges to that of $\mathbf{Y}^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}$. Let $\mathbf{Z} = \boldsymbol{\Sigma}^{-1/2}\mathbf{Y} \sim N_p(\mathbf{0}, I)$. Then $\mathbf{Y}^T \boldsymbol{\Sigma}^{-1} \mathbf{Y} = \mathbf{Z}^T\mathbf{Z} \sim \chi^2_p$.

\subsection*{Question 9}
\textbf{Question:} What is the one-sample Hotelling's $T^2$ test used for? State the null hypothesis and the test statistic.

\textbf{Solution:}
It is used to test $H_0: \boldsymbol{\mu} = \boldsymbol{\mu}_0$ vs. $H_1: \boldsymbol{\mu} \neq \boldsymbol{\mu}_0$. The test statistic is $T^2 = n(\bar{\mathbf{X}} - \boldsymbol{\mu}_0)^T S^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu}_0)$.

\subsection*{Question 10}
\textbf{Question:} What is the two-sample Hotelling's $T^2$ test used for? State the null hypothesis and assumptions.

\textbf{Solution:}
It is used to test $H_0: \boldsymbol{\mu}_1 = \boldsymbol{\mu}_2$. It assumes two independent random samples from multivariate normal populations with a common covariance matrix $\Sigma_1 = \Sigma_2 = \Sigma$.

\subsection*{Question 11}
\textbf{Question:} Let $\mathbf{X}_1, \dots, \mathbf{X}_n$ be a random sample from $\mathcal{N}_3(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. For testing $H_0: \mu_1 = \mu_2 - \mu_3$, construct a test based on Hotelling's $T^2$ statistic.

\textbf{Solution:}
The hypothesis is $H_0: \mathbf{C}\boldsymbol{\mu} = 0$, where $\mathbf{C} = (1, -1, 1)$. Let $\mathbf{Y}_i = \mathbf{C}\mathbf{X}_i$. The problem reduces to a one-sample test for the mean of the univariate random variable $Y$. The statistic is $T^2 = n(\mathbf{C}\bar{\mathbf{X}})^T(\mathbf{C}S\mathbf{C}^T)^{-1}(\mathbf{C}\bar{\mathbf{X}})$, which is equivalent to a standard t-test.

\subsection*{Question 12}
\textbf{Question:} Let a population distribution be $N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. Based on a random sample of size n, develop a large sample LRT for $H_0: \boldsymbol{\Sigma} = \boldsymbol{\Sigma}_0$.

\textbf{Solution:}
The LRT statistic is $\Lambda = \frac{\sup_{\boldsymbol{\mu}} L(\boldsymbol{\mu}, \Sigma_0)}{\sup_{\boldsymbol{\mu}, \Sigma} L(\boldsymbol{\mu}, \Sigma)}$. It can be shown that $-2\ln\Lambda = n(\text{tr}(\hat{\Sigma}\Sigma_0^{-1}) - \ln(|\hat{\Sigma}\Sigma_0^{-1}|) - p)$, which is approximately $\chi^2_{p(p+1)/2}$ for large $n$.

\subsection*{Question 13}
\textbf{Question:} Explain Bonferroni simultaneous confidence intervals for $\mu_1, \dots, \mu_p$ and compare them to $T^2$ intervals.

\textbf{Solution:}
To get a $100(1-\alpha)\%$ simultaneous confidence level, construct each of the $p$ individual intervals at the $100(1-\alpha/p)\%$ level using a t-distribution critical value: $\bar{x}_i \pm t_{n-1, \alpha/(2p)} \sqrt{s_{ii}/n}$. Bonferroni intervals are generally wider (more conservative) than $T^2$ intervals but are more flexible and easier to compute.

\subsection*{Question 14}
\textbf{Question:} Derive the two-sample Hotelling's $T^2$ statistic using the likelihood ratio test framework.

\textbf{Solution:}
The LRT for $H_0: \boldsymbol{\mu}_1 = \boldsymbol{\mu}_2$ (assuming $\Sigma_1=\Sigma_2=\Sigma$) is $\Lambda = \frac{\sup_{\boldsymbol{\mu}, \Sigma} L(\boldsymbol{\mu}, \Sigma)}{\sup_{\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \Sigma} L(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \Sigma)}$. The resulting statistic $\Lambda^{2/n}$ is a monotonic function of the two-sample Hotelling's $T^2 = \frac{n_1 n_2}{n_1+n_2}(\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)^T S_{pooled}^{-1} (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)$.

\subsection*{Question 15}
\textbf{Question:} How does Hotelling's $T^2$ statistic relate to the Mahalanobis distance?

\textbf{Solution:}
Hotelling's $T^2$ statistic is proportional to the squared Mahalanobis distance. Specifically, the one-sample $T^2$ is $n$ times the squared Mahalanobis distance between the sample mean vector $\bar{\mathbf{X}}$ and the hypothesized mean vector $\boldsymbol{\mu}_0$, using the sample covariance matrix $S$ to estimate the population covariance.
$$ T^2 = n \cdot D_M^2(\bar{\mathbf{X}}, \boldsymbol{\mu}_0) $$

\subsection*{Question 16}
\textbf{Question:} For a random sample from a $N_p(\boldsymbol{\mu}, \Sigma)$ population, derive the likelihood ratio test (LRT) for the hypothesis $H_0: \boldsymbol{\mu} = \boldsymbol{\mu}_0$ vs $H_1: \boldsymbol{\mu} \neq \boldsymbol{\mu}_0$, with $\Sigma$ unknown. Show that the LRT statistic is a monotonic function of Hotelling's $T^2$ statistic.

\textbf{Solution:}
The LRT statistic is $\Lambda = \left(\frac{|\hat{\Sigma}|}{|\hat{\Sigma}_0|}\right)^{n/2}$. We can show that $\frac{|\hat{\Sigma}_0|}{|\hat{\Sigma}|} = 1 + \frac{T^2}{n-1}$, where $T^2 = n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)^T S^{-1} (\bar{\mathbf{X}}-\boldsymbol{\mu}_0)$. So, $\Lambda^{2/n} = \left(1 + \frac{T^2}{n-1}\right)^{-1}$. Since $\Lambda$ is a decreasing function of $T^2$, the tests are equivalent.

\subsection*{Question 17}
\textbf{Question:} Compare the Mean Squared Error (MSE) of the MLE for variance, $\hat{\sigma}^2 = \frac{1}{n}\sum(X_i-\bar{X})^2$, and the sample variance, $s^2 = \frac{1}{n-1}\sum(X_i-\bar{X})^2$.

\textbf{Solution:}
$\text{MSE}(s^2) = \frac{2\sigma^4}{n-1}$ and $\text{MSE}(\hat{\sigma}^2) = \frac{(2n-1)\sigma^4}{n^2}$. For $n>1$, $\text{MSE}(s^2) > \text{MSE}(\hat{\sigma}^2)$, so the MLE is preferred in terms of MSE.

\subsection*{Question 18}
\textbf{Question:} Propose a test statistic for the multivariate Behrens-Fisher problem ($H_0: \boldsymbol{\mu}_1 = \boldsymbol{\mu}_2$ when $\Sigma_1 \neq \Sigma_2$) and explain the difficulty.

\textbf{Solution:}
For large samples, a reasonable test statistic is $T^2_{BF} = (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)^T \left( \frac{S_1}{n_1} + \frac{S_2}{n_2} \right)^{-1} (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)$, which is approximately $\chi^2_p$. The difficulty for small samples is that the exact distribution of this statistic is unknown.

\subsection*{Question 19}
\textbf{Question:} Derive the LRT for the sphericity hypothesis $H_0: \Sigma = \sigma^2 I$.

\textbf{Solution:}
The LRT statistic is $\Lambda = \left( \frac{|\hat{\Sigma}|}{(\frac{\text{tr}(\hat{\Sigma})}{p})^p} \right)^{n/2}$. This is a power of the ratio of the geometric mean to the arithmetic mean of the eigenvalues of $\hat{\Sigma}$.

\subsection*{Question 20}
\textbf{Question:} Discuss the concept of power for the one-sample Hotelling's $T^2$ test.

\textbf{Solution:}
Power is the probability of correctly rejecting $H_0$. For Hotelling's $T^2$, it depends on the non-centrality parameter $\delta^2 = n(\boldsymbol{\mu} - \boldsymbol{\mu}_0)^T \Sigma^{-1} (\boldsymbol{\mu} - \boldsymbol{\mu}_0)$. Power increases with $\delta^2$.

\subsection*{Question 21}
\textbf{Question:} To test for an "equal correlation" structure, $H_0: \mathbf{P} = (1-\rho)I + \rho\mathbf{1}\mathbf{1}^T$, the test statistic for a large sample is given by $T = \frac{n-1}{1-\bar{r}^2}((\sum\sum_{i<k} (r_{ik} - \bar{r})^2) - \gamma \sum_{k=1}^p (\bar{r}_k - \bar{r})^2)$, where $\gamma = \frac{p(p-1)}{2(p-2)^2}$ and other terms are sample estimates. It is known that $T \sim \chi^2_{(p+1)(p-2)/2}$. Use this to test $H_0$ at 1\% level of significance for a sample of size 150 with correlation matrix $R = \begin{pmatrix} 1 & .7501 & 0.6392 & 0.6363 \\ .7501 & 1 & 0.6925 & 0.7386 \\ 0.6392 & 0.6925 & 1 & 0.6625 \\ 0.6363 & 0.7386 & 0.6625 & 1 \end{pmatrix}$.

\textbf{Solution:}
Here $p=4$ and $n=150$. We calculate the mean correlation $\bar{r} \approx 0.6865$. We then calculate the sum of squared differences of correlations from the mean, and the sum of squared differences of column-average correlations from the mean. Plugging these values into the formula for $T$ gives $T \approx 2.41$. The degrees of freedom are $(4+1)(4-2)/2 = 5$. The critical value is $\chi^2_{5, 0.01} = 15.086$. Since $2.41 < 15.086$, we do not reject $H_0$.

\subsection*{Question 22}
\textbf{Question:} (True/False) For any fixed vector $\mathbf{d} \in \mathbb{R}^p$, the distribution of the squared Mahalanobis distance $(\mathbf{d} - \mathbf{X})^T \boldsymbol{\Sigma}^{-1} (\mathbf{d} - \mathbf{X})$, where $\mathbf{X} \sim \mathcal{N}_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, has a non-central chi-square distribution. Justify your answer.

\textbf{Solution:}
True. Let $\mathbf{Y} = \mathbf{X} - \mathbf{d} \sim N_p(\boldsymbol{\mu}-\mathbf{d}, \boldsymbol{\Sigma})$. The quadratic form is $\mathbf{Y}^T\boldsymbol{\Sigma}^{-1}\mathbf{Y}$. Let $\mathbf{Z} = \boldsymbol{\Sigma}^{-1/2}\mathbf{Y} \sim N_p(\boldsymbol{\Sigma}^{-1/2}(\boldsymbol{\mu}-\mathbf{d}), I)$. The form is $\mathbf{Z}^T\mathbf{Z}$, which is a sum of squared normal variables with unit variance and non-zero mean. This follows a non-central chi-square distribution, $\chi^2_p(\delta)$, with non-centrality parameter $\delta^2 = (\boldsymbol{\mu}-\mathbf{d})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}-\mathbf{d})$.

\subsection*{Question 23}
\textbf{Question:} Let $\mathbf{A} \sim W_p(n, \boldsymbol{\Sigma})$. Then using the definition of Wishart distribution or otherwise, prove or disprove $\mathbb{E}(\mathbf{A}) = n\boldsymbol{\Sigma}$.

\textbf{Solution:}
This is true.
By the definition of the Wishart distribution, the matrix $\mathbf{A}$ can be represented as the sum of $n$ independent outer products of random vectors, $\mathbf{A} = \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T$, where each $\mathbf{X}_i \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$.
We want to compute the expectation of $\mathbf{A}$. By the linearity of expectation:
$$ \mathbb{E}(\mathbf{A}) = \mathbb{E}\left[\sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T\right] = \sum_{i=1}^n \mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T] $$
The expectation $\mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T]$ is the second moment matrix of the random vector $\mathbf{X}_i$. This is related to the covariance matrix by $\text{Cov}(\mathbf{X}_i) = \mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T] - \mathbb{E}[\mathbf{X}_i]\mathbb{E}[\mathbf{X}_i]^T$.
Since $\mathbf{X}_i \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$, its mean is $\mathbb{E}[\mathbf{X}_i] = \mathbf{0}$ and its covariance is $\text{Cov}(\mathbf{X}_i) = \boldsymbol{\Sigma}$.
Therefore, $\mathbb{E}[\mathbf{X}_i \mathbf{X}_i^T] = \text{Cov}(\mathbf{X}_i) + \mathbf{0}\mathbf{0}^T = \boldsymbol{\Sigma}$.
Substituting this back into the sum:
$$ \mathbb{E}(\mathbf{A}) = \sum_{i=1}^n \boldsymbol{\Sigma} = n\boldsymbol{\Sigma} $$
The proof is complete.

\subsection*{Question 24}
\textbf{Question:} Let $\mathbf{x}_1, \dots, \mathbf{x}_n$ be a random sample from $\mathcal{N}_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. Let $\bar{\mathbf{X}}$ and $\mathbf{S}$ be the sample mean and covariance matrix. Prove or disprove that the distribution of $n(\bar{\mathbf{X}} - \boldsymbol{\mu})^T \mathbf{S}^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu})$ converges to a chi-square distribution as $n \to \infty$.

\textbf{Solution:}
True.
1.  By the multivariate Central Limit Theorem, we know that $\sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu})$ converges in distribution to a multivariate normal random vector $\mathbf{Y} \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$.
2.  By the Law of Large Numbers, the sample covariance matrix $\mathbf{S}$ is a consistent estimator of the true covariance matrix $\boldsymbol{\Sigma}$. This means $\mathbf{S}$ converges in probability to $\boldsymbol{\Sigma}$ as $n \to \infty$, and therefore $\mathbf{S}^{-1} \xrightarrow{p} \boldsymbol{\Sigma}^{-1}$.
3.  Let $\mathbf{Y}_n = \sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu})$. The statistic is $\mathbf{Y}_n^T \mathbf{S}^{-1} \mathbf{Y}_n$.
4.  By Slutsky's Theorem, if $\mathbf{Y}_n \xrightarrow{d} \mathbf{Y}$ and $\mathbf{S}^{-1} \xrightarrow{p} \boldsymbol{\Sigma}^{-1}$, then the joint distribution of $(\mathbf{Y}_n, \mathbf{S}^{-1})$ converges to that of $(\mathbf{Y}, \boldsymbol{\Sigma}^{-1})$.
5.  By the continuous mapping theorem, the distribution of the statistic converges to the distribution of $\mathbf{Y}^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}$.
6.  This resulting quadratic form, where $\mathbf{Y} \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$, is the definition of a chi-square random variable with $p$ degrees of freedom, $\chi^2_p$.

\subsection*{Question 25}
\textbf{Question:} Show that an approximate distribution of $n(\bar{\mathbf{X}} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu})$ is a $\chi^2$-distribution with p-degrees of freedom for a large $n-p$, where $\bar{\mathbf{X}}$ is the sample mean vector of a random sample of size n from any population having covariance matrix $\boldsymbol{\Sigma}$.

\textbf{Solution:}
This is a direct application of the multivariate Central Limit Theorem.
1.  Let $\mathbf{Y}_n = \sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu})$. The CLT states that as $n \to \infty$, the distribution of $\mathbf{Y}_n$ approaches a multivariate normal distribution $N_p(\mathbf{0}, \boldsymbol{\Sigma})$, regardless of the underlying population distribution (as long as it has a finite covariance matrix).
2.  The statistic in question is $T_n = (\sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu}))^T \boldsymbol{\Sigma}^{-1} (\sqrt{n}(\bar{\mathbf{X}} - \boldsymbol{\mu})) = \mathbf{Y}_n^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}_n$.
3.  By the continuous mapping theorem, as $\mathbf{Y}_n \xrightarrow{d} \mathbf{Y} \sim N_p(\mathbf{0}, \boldsymbol{\Sigma})$, the distribution of the continuous function $g(\mathbf{Y}_n) = \mathbf{Y}_n^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}_n$ converges to the distribution of $g(\mathbf{Y}) = \mathbf{Y}^T \boldsymbol{\Sigma}^{-1} \mathbf{Y}$.
4.  Let $\mathbf{Z} = \boldsymbol{\Sigma}^{-1/2}\mathbf{Y} \sim N_p(\mathbf{0}, I)$. Then $\mathbf{Y}^T\boldsymbol{\Sigma}^{-1}\mathbf{Y} = (\boldsymbol{\Sigma}^{1/2}\mathbf{Z})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{\Sigma}^{1/2}\mathbf{Z}) = \mathbf{Z}^T\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}^{1/2}\mathbf{Z} = \mathbf{Z}^T I \mathbf{Z} = \mathbf{Z}^T\mathbf{Z}$.
5.  $\mathbf{Z}^T\mathbf{Z} = \sum_{i=1}^p Z_i^2$, where $Z_i$ are independent $N(0,1)$ variables. This is the definition of a chi-square distribution with $p$ degrees of freedom.
Thus, for large $n$, the statistic is approximately distributed as $\chi^2_p$.

\subsection*{Question 26}
\textbf{Question:} Suppose $\binom{1}{2}, \binom{2}{-2}, \binom{-2}{1}, \binom{1}{-3}, \binom{-2}{3}$ are five observations from a 2-dimensional Normal distribution with mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$. Use the above data to explain what is Wishart distribution.

\textbf{Solution:}
The Wishart distribution describes the distribution of the sample sum of squares and cross-products (SSCP) matrix.
1.  **Calculate the sample mean:**
    $\bar{x}_1 = (1+2-2+1-2)/5 = 0$.
    $\bar{x}_2 = (2-2+1-3+3)/5 = 0.2$.
    $\bar{\mathbf{x}} = \binom{0}{0.2}$.
2.  **Calculate the deviation vectors:**
    $\mathbf{d}_1 = \binom{1}{1.8}, \mathbf{d}_2 = \binom{2}{-2.2}, \mathbf{d}_3 = \binom{-2}{0.8}, \mathbf{d}_4 = \binom{1}{-3.2}, \mathbf{d}_5 = \binom{-2}{2.8}$.
3.  **Calculate the SSCP matrix:**
    The SSCP matrix is $A = \sum_{i=1}^5 \mathbf{d}_i \mathbf{d}_i^T$.
    For example, $\mathbf{d}_1\mathbf{d}_1^T = \binom{1}{1.8}\binom{1}{1.8}^T = \begin{pmatrix} 1 & 1.8 \\ 1.8 & 3.24 \end{pmatrix}$.
    Summing these five outer product matrices gives the observed SSCP matrix $A$.
4.  **Explain the Distribution:**
    If the original data came from a $N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ distribution, then the matrix $A$ that we calculated is a single realization from a Wishart distribution with $n-1=4$ degrees of freedom and scale matrix $\boldsymbol{\Sigma}$. We write this as $A \sim W_2(4, \boldsymbol{\Sigma})$.

\subsection*{Question 27}
\textbf{Question:} Three observations from $\mathcal{N}_2(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ are (1,2), (1.2, 1.8), (2,3). Find a 95\% minimum volume confidence region for $\boldsymbol{\mu}$.

\textbf{Solution:}
First calculate $\bar{\mathbf{x}} = (1.4, 2.267)^T$ and $\mathbf{S} = \begin{pmatrix} 0.28 & 0.38 \\ 0.38 & 0.523 \end{pmatrix}$.
The confidence region is given by $n(\bar{\mathbf{x}}-\boldsymbol{\mu})^T \mathbf{S}^{-1} (\bar{\mathbf{x}}-\boldsymbol{\mu}) \le \frac{p(n-1)}{n-p}F_{p,n-p,\alpha}$.
Here $n=3, p=2, \alpha=0.05$. We need $F_{2,1,0.05} = 199.5$.
The critical value is $\frac{2(2)}{1} \cdot 199.5 = 798$.
The confidence region is the ellipse defined by $3((\bar{\mathbf{x}}-\boldsymbol{\mu})^T \mathbf{S}^{-1} (\bar{\mathbf{x}}-\boldsymbol{\mu})) \le 798$.

\subsection*{Question 28}
\textbf{Question:} Let $\mathbf{X}_1, \dots, \mathbf{X}_n$ be a random sample from $\mathcal{N}_3(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. For testing $H_0: \boldsymbol{\mu} = (1, 2, 3)^T$, construct a test based on Hotelling's $T^2$ statistic and write its distribution under $H_0$.

\textbf{Solution:}
The test statistic is Hotelling's one-sample $T^2$:
$$ T^2 = n(\bar{\mathbf{X}} - \boldsymbol{\mu}_0)^T S^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu}_0) $$
where $\boldsymbol{\mu}_0 = (1,2,3)^T$.
Under $H_0$, this statistic is distributed as a scaled F-distribution:
$$ \frac{n-p}{p(n-1)} T^2 \sim F_{p, n-p} $$
Here, $p=3$, so the distribution is $\frac{n-3}{3(n-1)} T^2 \sim F_{3, n-3}$. We reject $H_0$ for large values of $T^2$.

\subsection*{Question 29}
\textbf{Question:} Let $\mathbf{X}_1, \dots, \mathbf{X}_n$ be a random sample from $\mathcal{N}_3(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. For testing $H_0: \mu_1 = \mu_2 - \mu_3$, construct a test based on Hotelling's $T^2$ statistic and write its distribution under $H_0$.

\textbf{Solution:}
This hypothesis can be written in the form $H_0: \mathbf{C}\boldsymbol{\mu} = \mathbf{0}$, where $\mathbf{C}$ is a contrast matrix, $\mathbf{C} = \begin{pmatrix} 1 & -1 & 1 \end{pmatrix}$.
Let $\mathbf{Y}_i = \mathbf{C}\mathbf{X}_i$. Then $\bar{\mathbf{Y}} = \mathbf{C}\bar{\mathbf{X}}$ and the sample covariance is $S_Y = \mathbf{C}S\mathbf{C}^T$.
Since the linear combination is a scalar, this reduces to a one-sample test for a univariate mean. The Hotelling's $T^2$ statistic for this hypothesis is:
$$ T^2 = n(\mathbf{C}\bar{\mathbf{X}} - \mathbf{0})^T (S_Y)^{-1} (\mathbf{C}\bar{\mathbf{X}} - \mathbf{0}) = n(\mathbf{C}\bar{\mathbf{X}})^T (\mathbf{C}S\mathbf{C}^T)^{-1} (\mathbf{C}\bar{\mathbf{X}}) $$
This is equivalent to a standard one-sample t-test, where $t = \frac{\mathbf{C}\bar{\mathbf{X}}}{\sqrt{S_Y/n}}$. The distribution of the $T^2$ statistic is $\frac{n-1}{1(n-1)}T^2 \sim F_{1, n-1}$, which is the square of a t-distribution with $n-1$ degrees of freedom.

\subsection*{Question 30}
\textbf{Question:} For sample data with $\bar{\mathbf{x}} = (1,0,2)^T$ and $S = \begin{pmatrix} 3 & 2 & 1 \\ 2 & 3 & 1 \\ 1 & 1 & 4 \end{pmatrix}$ from a sample of size 10, can we assert that $\mu_1 = (\mu_2+\mu_3)/2$? Test at a 5\% level.

\textbf{Solution:}
The hypothesis is $H_0: \mu_1 - 0.5\mu_2 - 0.5\mu_3 = 0$. This is of the form $\mathbf{C}\boldsymbol{\mu}=0$ with $\mathbf{C}=(1, -0.5, -0.5)$. We construct a t-test.
$\mathbf{C}\bar{\mathbf{x}} = 1 - 0 - 1 = 0$.
$\mathbf{C}S\mathbf{C}^T = (1, -0.5, -0.5) S (1, -0.5, -0.5)^T = 2.25$.
The t-statistic is $t = \frac{\sqrt{n}\mathbf{C}\bar{\mathbf{x}}}{\sqrt{\mathbf{C}S\mathbf{C}^T}} = \frac{\sqrt{10} \cdot 0}{\sqrt{2.25}} = 0$.
The critical value is $t_{9, 0.025} = 2.262$. Since $|t| < 2.262$, we do not reject $H_0$. We can assert that the relationship holds.

\subsection*{Question 31}
\textbf{Question:} Define the 2-sample Hotelling $T^2$-statistic for testing equality of mean vectors from two normal populations, and find its distribution.

\textbf{Solution:}
Let there be two independent random samples of size $n_1$ and $n_2$ from $N_p(\boldsymbol{\mu}_1, \boldsymbol{\Sigma})$ and $N_p(\boldsymbol{\mu}_2, \boldsymbol{\Sigma})$ respectively.
The test statistic for $H_0: \boldsymbol{\mu}_1 = \boldsymbol{\mu}_2$ is:
$$ T^2 = \frac{n_1 n_2}{n_1+n_2}(\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)^T S_{pooled}^{-1} (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2) $$
where $S_{pooled} = \frac{(n_1-1)S_1 + (n_2-1)S_2}{n_1+n_2-2}$ is the pooled covariance matrix.
The distribution of the statistic under $H_0$ is:
$$ \frac{n_1+n_2-p-1}{p(n_1+n_2-2)} T^2 \sim F_{p, n_1+n_2-p-1} $$

\subsection*{Question 32}
\textbf{Question:} For 42 observations on two variables, $\bar{\mathbf{x}} = \binom{0.564}{0.603}$ and $\mathbf{S} = \begin{pmatrix} 0.0144 & 0.0117 \\ 0.0117 & 0.0146 \end{pmatrix}$. Test $H_0: \boldsymbol{\mu} = (0.5, 0.6)^T$ at $\alpha=0.05$.

\textbf{Solution:}
$n=42, p=2, \boldsymbol{\mu}_0 = (0.5, 0.6)^T$.
$\bar{\mathbf{x}}-\boldsymbol{\mu}_0 = \binom{0.064}{0.003}$.
$S^{-1} = \frac{1}{0.00007323}\begin{pmatrix} 0.0146 & -0.0117 \\ -0.0117 & 0.0144 \end{pmatrix}$.
$T^2 = n(\bar{\mathbf{x}}-\boldsymbol{\mu}_0)^T S^{-1} (\bar{\mathbf{x}}-\boldsymbol{\mu}_0) \approx 42 \cdot 13655 \cdot (0.0034) \approx 1948$.
The critical value is $\frac{p(n-1)}{n-p}F_{p,n-p,\alpha} = \frac{2(41)}{40}F_{2,40,0.05} \approx 2.05 \cdot 3.23 = 6.62$.
Since $T^2$ is much larger than the critical value, we reject $H_0$.

\subsection*{Question 33}
\textbf{Question:} For an iid sample of size n=5 from an $N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ distribution with $\boldsymbol{\Sigma} = \begin{pmatrix} 3 & \eta \\ \eta & 1 \end{pmatrix}$, where $\eta$ is known, and $\bar{\mathbf{x}}=\binom{1}{0}$. For what value of $\eta$ would $H_0: \boldsymbol{\mu} = (0,0)^T$ be rejected at the 5\% level?

\textbf{Solution:}
The test statistic is $T^2 = n(\bar{\mathbf{x}}-\boldsymbol{\mu}_0)^T \Sigma^{-1} (\bar{\mathbf{x}}-\boldsymbol{\mu}_0) \sim \chi^2_p$.
$T^2 = 5 \binom{1}{0}^T \frac{1}{3-\eta^2}\begin{pmatrix} 1 & -\eta \\ -\eta & 3 \end{pmatrix} \binom{1}{0} = \frac{5}{3-\eta^2}$.
The critical value is $\chi^2_{2, 0.05} = 5.99$.
We reject if $T^2 > 5.99 \implies \frac{5}{3-\eta^2} > 5.99 \implies 5 > 5.99(3-\eta^2) \implies 0.8347 > 3-\eta^2 \implies \eta^2 > 2.165$.
So, $|\eta| > \sqrt{2.165} \approx 1.47$. Also note that for $\Sigma$ to be positive definite, $|\Sigma|=3-\eta^2 > 0$, so $|\eta|<\sqrt{3}\approx 1.732$.
Thus, the hypothesis is rejected if $1.47 < |\eta| < 1.732$.
