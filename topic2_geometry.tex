\subsection*{Question 1}
\textbf{Question:} Explain the concept of a sample space in the context of multivariate analysis. How does it differ from the feature space?

\textbf{Solution:}
In multivariate analysis, the sample space is an $n$-dimensional space where $n$ is the number of observations. Each of the $p$ variables can be represented as a vector in this space. So, we have $p$ vectors in an $n$-dimensional space. This is in contrast to the feature space, which is a $p$-dimensional space where each of the $n$ observations is represented as a point.

\subsection*{Question 2}
\textbf{Question:} What is a vector projection? Provide the formula for projecting a vector $\mathbf{y}$ onto a vector $\mathbf{x}$.

\textbf{Solution:}
A vector projection of a vector $\mathbf{y}$ onto a vector $\mathbf{x}$ is the component of $\mathbf{y}$ that lies in the direction of $\mathbf{x}$. The formula is:
$$ \text{proj}_{\mathbf{x}} \mathbf{y} = \frac{\mathbf{y}^T \mathbf{x}}{\mathbf{x}^T \mathbf{x}} \mathbf{x} $$
This gives a vector in the direction of $\mathbf{x}$. The scalar value $\frac{\mathbf{y}^T \mathbf{x}}{\mathbf{x}^T \mathbf{x}}$ is the coordinate of the projection.

\subsection*{Question 3}
\textbf{Question:} Given vectors $\mathbf{y} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$ and $\mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$, find the projection of $\mathbf{y}$ onto $\mathbf{x}$.

\textbf{Solution:}
We use the formula for projection:
$$ \mathbf{y}^T \mathbf{x} = (3)(1) + (4)(1) = 7 $$
$$ \mathbf{x}^T \mathbf{x} = (1)^2 + (1)^2 = 2 $$
$$ \text{proj}_{\mathbf{x}} \mathbf{y} = \frac{7}{2} \mathbf{x} = \frac{7}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 3.5 \\ 3.5 \end{pmatrix} $$

\subsection*{Question 4}
\textbf{Question:} How can we interpret the length of a vector in Euclidean space? What does the squared length of a mean-centered vector represent?

\textbf{Solution:}
The length of a vector $\mathbf{x} = (x_1, ..., x_n)^T$ is given by $L_{\mathbf{x}} = \sqrt{\sum_{i=1}^n x_i^2}$.
A mean-centered vector $\mathbf{d} = \mathbf{x} - \bar{x}\mathbf{1}$ has elements $d_i = x_i - \bar{x}$. The squared length of this vector is:
$$ L_{\mathbf{d}}^2 = \sum_{i=1}^n (x_i - \bar{x})^2 $$
This is $(n-1)$ times the sample variance of the variable $x$.

\subsection*{Question 5}
\textbf{Question:} Calculate the length of the vector $\mathbf{d} = \mathbf{y} - \bar{y}\mathbf{1}$, where $\mathbf{y} = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$ and $\mathbf{1} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$.

\textbf{Solution:}
First, calculate the mean $\bar{y} = (1+2+3)/3 = 2$.
The mean-centered vector is:
$$ \mathbf{d} = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} - 2 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix} $$
The length of $\mathbf{d}$ is:
$$ L_{\mathbf{d}} = \sqrt{(-1)^2 + 0^2 + 1^2} = \sqrt{2} $$

\subsection*{Question 6}
\textbf{Question:} Define the cosine of the angle between two vectors. What do values of 1, 0, and -1 signify?

\textbf{Solution:}
The cosine of the angle $\theta$ between two vectors $\mathbf{x}$ and $\mathbf{y}$ is:
$$ \cos(\theta) = \frac{\mathbf{x}^T \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|} $$
- $\cos(\theta) = 1$ means the vectors point in the same direction ($\theta=0^\circ$).
- $\cos(\theta) = 0$ means the vectors are orthogonal ($\theta=90^\circ$).
- $\cos(\theta) = -1$ means the vectors point in opposite directions ($\theta=180^\circ$).

\subsection*{Question 7}
\textbf{Question:} Find the cosine of the angle between the two vectors from Question 3, $\mathbf{y} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$ and $\mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.

\textbf{Solution:}
$$ \mathbf{x}^T \mathbf{y} = 7 $$
$$ \|\mathbf{x}\| = \sqrt{1^2+1^2} = \sqrt{2} $$
$$ \|\mathbf{y}\| = \sqrt{3^2+4^2} = \sqrt{25} = 5 $$
$$ \cos(\theta) = \frac{7}{5\sqrt{2}} \approx 0.9899 $$

\subsection*{Question 8}
\textbf{Question:} How does the concept of cosine angle relate to the sample correlation coefficient?

\textbf{Solution:}
The sample correlation coefficient $r$ between two variables $x$ and $y$ is the cosine of the angle between their mean-centered vectors in the $n$-dimensional sample space. If $\mathbf{d}_x = \mathbf{x} - \bar{x}\mathbf{1}$ and $\mathbf{d}_y = \mathbf{y} - \bar{y}\mathbf{1}$, then:
$$ r_{xy} = \frac{\mathbf{d}_x^T \mathbf{d}_y}{\|\mathbf{d}_x\| \|\mathbf{d}_y\|} = \cos(\theta) $$

\subsection*{Question 9}
\textbf{Question:} Explain how a linear combination of variables can be viewed as a projection.

\textbf{Solution:}
Consider a linear combination of $p$ variables, $c_1 x_1 + ... + c_p x_p$. In the sample space, we have $p$ vectors $\mathbf{x}_1, ..., \mathbf{x}_p$. The linear combination forms a new vector $\mathbf{z} = c_1 \mathbf{x}_1 + ... + c_p \mathbf{x}_p$. This vector $\mathbf{z}$ lies in the subspace spanned by the original variable vectors. Each observation's value for this new variable, $z_i$, is its value on the new axis defined by the linear combination. This can be seen as a projection of the observation points onto this new axis.

\subsection*{Question 10}
\textbf{Question:} Project the first observation vector $\mathbf{x}_1 = \begin{pmatrix} 2 \\ 3 \end{pmatrix}$ from the feature space onto the vector $\mathbf{v} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$.

\textbf{Solution:}
This is a projection in the feature space.
$$ \mathbf{x}_1^T \mathbf{v} = (2)(1) + (3)(-1) = -1 $$
$$ \mathbf{v}^T \mathbf{v} = (1)^2 + (-1)^2 = 2 $$
$$ \text{proj}_{\mathbf{v}} \mathbf{x}_1 = \frac{-1}{2} \mathbf{v} = \begin{pmatrix} -0.5 \\ 0.5 \end{pmatrix} $$

\subsection*{Question 11}
\textbf{Question:} What is the geometric interpretation of the sample variance?

\textbf{Solution:}
Geometrically, the sample variance of a variable is proportional to the squared length of its mean-centered vector in the sample space. A larger variance means the vector is longer, indicating more spread in the data.
$$ s_x^2 = \frac{1}{n-1} \sum (x_i - \bar{x})^2 = \frac{1}{n-1} \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 $$

\subsection*{Question 12}
\textbf{Question:} What is the geometric interpretation of the sample covariance?

\textbf{Solution:}
The sample covariance between two variables $x$ and $y$ is proportional to the dot product of their mean-centered vectors.
$$ s_{xy} = \frac{1}{n-1} (\mathbf{x} - \bar{x}\mathbf{1})^T (\mathbf{y} - \bar{y}\mathbf{1}) $$
The sign of the covariance is determined by the angle between these vectors. If the angle is less than 90 degrees, the covariance is positive. If it's greater than 90 degrees, it's negative.

\subsection*{Question 13}
\textbf{Question:} Given two mean-centered vectors $\mathbf{d}_1 = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}$ and $\mathbf{d}_2 = \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}$, calculate their dot product. What does this imply about their sample covariance?

\textbf{Solution:}
$$ \mathbf{d}_1^T \mathbf{d}_2 = (-1)(-1) + (0)(1) + (1)(0) = 1 $$
Since the dot product is positive, the sample covariance between the two corresponding variables is positive. The sample covariance would be $1 / (n-1) = 1/2$.

\subsection*{Question 14}
\textbf{Question:} Describe how you would find a projection of a data set that maximizes the variance of the projected points.

\textbf{Solution:}
This is the core idea of Principal Component Analysis (PCA). We want to find a direction (a unit vector $\mathbf{a}$) such that when we project the data points onto this direction, the variance of the projected points is maximized. The projected values are given by $X\mathbf{a}$. The variance of these projected values is proportional to $\mathbf{a}^T S \mathbf{a}$, where $S$ is the covariance matrix. To maximize this quantity subject to $\|\mathbf{a}\|=1$, we find the eigenvector of $S$ corresponding to the largest eigenvalue. This eigenvector is the direction of maximum variance.

\subsection*{Question 15}
\textbf{Question:} If two vectors representing two variables are orthogonal in the sample space after being mean-centered, what does this imply about their correlation?

\textbf{Solution:}
If the mean-centered vectors $\mathbf{d}_x$ and $\mathbf{d}_y$ are orthogonal, their dot product is zero: $\mathbf{d}_x^T \mathbf{d}_y = 0$. Since the sample correlation is the cosine of the angle between these vectors, and the angle is 90 degrees, the correlation is $\cos(90^\circ) = 0$. This means the two variables are uncorrelated.

\subsection*{Question 16}
\textbf{Question:} Let $X_c$ be the $n \times p$ mean-centered data matrix. The projection of the $n$ observation points onto a p-dimensional vector $\mathbf{a}$ results in a new vector of data points $X_c \mathbf{a}$. Show that the sample variance of these projected points is given by $\mathbf{a}^T S \mathbf{a}$.

\textbf{Solution:}
The vector of projected points is $\mathbf{z} = X_c \mathbf{a}$. The elements of $\mathbf{z}$ are $z_i = (\mathbf{x}_i - \bar{\mathbf{x}})^T \mathbf{a}$.
The mean of the projected points is:
$$ \bar{z} = \frac{1}{n} \sum_{i=1}^n z_i = \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})^T \mathbf{a} = \left( \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})^T \right) \mathbf{a} = (\bar{\mathbf{x}} - \bar{\mathbf{x}})^T \mathbf{a} = 0 $$
So the projected points are already centered around 0.
The sample variance of the projected points is:
$$ s_z^2 = \frac{1}{n-1} \sum_{i=1}^n (z_i - \bar{z})^2 = \frac{1}{n-1} \sum_{i=1}^n z_i^2 = \frac{1}{n-1} (X_c \mathbf{a})^T (X_c \mathbf{a}) $$
$$ = \frac{1}{n-1} \mathbf{a}^T X_c^T X_c \mathbf{a} = \mathbf{a}^T \left( \frac{1}{n-1} X_c^T X_c \right) \mathbf{a} = \mathbf{a}^T S \mathbf{a} $$
This result is fundamental to Principal Component Analysis (PCA), where we seek to find the direction $\mathbf{a}$ that maximizes this variance.

\subsection*{Question 17}
\textbf{Question:} Let $\mathbf{d}_j$ and $\mathbf{d}_k$ be two mean-centered data vectors (columns of $X_c$) for variables $j$ and $k$. The simple linear regression coefficient of variable $j$ on variable $k$ is given by $b_{jk} = \frac{\sum_{i=1}^n d_{ij}d_{ik}}{\sum_{i=1}^n d_{ik}^2} = \frac{\mathbf{d}_j^T \mathbf{d}_k}{\mathbf{d}_k^T \mathbf{d}_k}$. Interpret this geometrically in terms of projections in the $n$-dimensional sample space.

\textbf{Solution:}
In the $n$-dimensional sample space, the vectors $\mathbf{d}_j$ and $\mathbf{d}_k$ represent the two variables. The formula for the regression coefficient $b_{jk}$ is identical to the scalar component of the projection of vector $\mathbf{d}_j$ onto vector $\mathbf{d}_k$.
The projection of $\mathbf{d}_j$ onto the line defined by $\mathbf{d}_k$ is:
$$ \text{proj}_{\mathbf{d}_k} \mathbf{d}_j = \left( \frac{\mathbf{d}_j^T \mathbf{d}_k}{\mathbf{d}_k^T \mathbf{d}_k} \right) \mathbf{d}_k = b_{jk} \mathbf{d}_k $$
Geometrically, this means we are finding the best fit for vector $\mathbf{d}_j$ in the one-dimensional subspace spanned by $\mathbf{d}_k$. The regression coefficient $b_{jk}$ is the scalar that tells us how much we need to scale $\mathbf{d}_k$ to get the vector that is "closest" to $\mathbf{d}_j$ in the least squares sense. The vector of predicted values, $\hat{\mathbf{d}}_j = b_{jk}\mathbf{d}_k$, is this projection.

\subsection*{Question 18}
\textbf{Question:} Consider the projection matrix $P = \frac{1}{n}\mathbf{1}\mathbf{1}^T$, where $\mathbf{1}$ is an $n \times 1$ vector of ones. Show that $P$ is idempotent ($P^2=P$) and symmetric. Interpret the geometric effect of applying $P$ and $(I-P)$ to a data vector $\mathbf{x}$ in the sample space.

\textbf{Solution:}
\textbf{Symmetry:}
$$ P^T = \left(\frac{1}{n}\mathbf{1}\mathbf{1}^T\right)^T = \frac{1}{n}(\mathbf{1}^T)^T\mathbf{1}^T = \frac{1}{n}\mathbf{1}\mathbf{1}^T = P $$
So, $P$ is symmetric.

\textbf{Idempotence:}
$$ P^2 = \left(\frac{1}{n}\mathbf{1}\mathbf{1}^T\right) \left(\frac{1}{n}\mathbf{1}\mathbf{1}^T\right) = \frac{1}{n^2} \mathbf{1}(\mathbf{1}^T\mathbf{1})\mathbf{1}^T $$
The term $\mathbf{1}^T\mathbf{1} = \sum_{i=1}^n 1^2 = n$.
$$ P^2 = \frac{1}{n^2} \mathbf{1}(n)\mathbf{1}^T = \frac{n}{n^2} \mathbf{1}\mathbf{1}^T = \frac{1}{n} \mathbf{1}\mathbf{1}^T = P $$
So, $P$ is idempotent.

\textbf{Geometric Interpretation:}
Applying $P$ to a data vector $\mathbf{x} = (x_1, \dots, x_n)^T$:
$$ P\mathbf{x} = \frac{1}{n}\mathbf{1}\mathbf{1}^T\mathbf{x} = \frac{1}{n}\mathbf{1} \left(\sum_{i=1}^n x_i\right) = \left(\frac{\sum x_i}{n}\right) \mathbf{1} = \bar{x}\mathbf{1} $$
This operation projects the vector $\mathbf{x}$ onto the vector $\mathbf{1}$. The result is a vector where every component is the mean of $\mathbf{x}$.

The matrix $H = (I-P)$ is the centering matrix. Applying it to $\mathbf{x}$:
$$ (I-P)\mathbf{x} = \mathbf{x} - P\mathbf{x} = \mathbf{x} - \bar{x}\mathbf{1} $$
This creates the mean-centered vector $\mathbf{d}_x$. Geometrically, this projects the vector $\mathbf{x}$ onto the subspace orthogonal to the vector $\mathbf{1}$. This subspace contains all vectors whose components sum to zero. $H$ is also a projection matrix (it is symmetric and idempotent).

\subsection*{Question 19}
\textbf{Question:} From a geometric perspective in the $n$-dimensional sample space, what is the goal of Canonical Correlation Analysis (CCA)? Describe the first canonical correlation in terms of finding vectors in this space.

\textbf{Solution:}
Canonical Correlation Analysis (CCA) aims to find the linear relationships between two sets of variables. Let the two sets of variables be represented by their mean-centered data vectors in the $n$-dimensional sample space: $\{\mathbf{d}_1, \dots, \mathbf{d}_p\}$ for the first set, and $\{\mathbf{g}_1, \dots, \mathbf{g}_q\}$ for the second set.

Geometrically, CCA seeks to find a pair of vectors, one in the subspace spanned by the first set of variables and one in the subspace spanned by the second set, such that the angle between these two new vectors is minimized. A smaller angle corresponds to a larger cosine, which represents a larger correlation.

Let $\mathbf{u} = c_1\mathbf{d}_1 + \dots + c_p\mathbf{d}_p$ be a vector in the subspace of the first set, and $\mathbf{v} = e_1\mathbf{g}_1 + \dots + e_q\mathbf{g}_q$ be a vector in the subspace of the second set. These are the canonical variates.

The first canonical correlation, $\rho_1^*$, is the maximum possible correlation between such a $\mathbf{u}$ and $\mathbf{v}$. Geometrically, it is the cosine of the minimum possible angle between the two subspaces. The vectors $\mathbf{u}_1$ and $\mathbf{v}_1$ that achieve this maximum correlation are the first pair of canonical variates. Subsequent canonical correlations are found by seeking pairs of vectors that are uncorrelated with (and geometrically, orthogonal to) the previous pairs and that have the next highest correlation.

\subsection*{Question 20}
\textbf{Question:} The canonical vectors $\mathbf{a}_1$ and $\mathbf{b}_1$ for CCA are found by solving the generalized eigenvalue problem $(S_{12}S_{22}^{-1}S_{21})\mathbf{a} = \lambda S_{11}\mathbf{a}$. Provide a geometric or statistical interpretation of the matrix product $M = S_{12}S_{22}^{-1}S_{21}$.

\textbf{Solution:}
Let's break down the matrix product $M = S_{12}S_{22}^{-1}S_{21}$. This matrix can be interpreted as the covariance matrix of the predicted values of the first set of variables, when they are predicted from the second set of variables using multivariate multiple regression.

1.  **Multivariate Regression:** Consider regressing the variables in the first set ($X_1$) onto the variables in the second set ($X_2$). The matrix of regression coefficients is given by $B = S_{12}S_{22}^{-1}$. The predicted values of $X_1$ from $X_2$ are $\hat{X}_1 = X_2 B^T = X_2 S_{22}^{-1}S_{21}$. (Note: In many texts, the regression of Y on X has coefficients $(X'X)^{-1}X'Y$. Here we regress X1 on X2, so the coefficients are $(S_{22})^{-1}S_{21}$).

2.  **Covariance of Predicted Values:** The matrix $M$ is the covariance matrix of these predicted values, $\text{Cov}(\hat{X}_1)$.
    $$ \text{Cov}(\hat{X}_1) = \text{Cov}(X_2 S_{22}^{-1}S_{21}) $$
    Using the property $\text{Cov}(XA^T) = A \text{Cov}(X) A^T$, with $A = (S_{22}^{-1}S_{21})^T = S_{12}S_{22}^{-1}$:
    $$ \text{Cov}(\hat{X}_1) = (S_{12}S_{22}^{-1}) \text{Cov}(X_2) (S_{22}^{-1}S_{21}) $$
    $$ = S_{12}S_{22}^{-1} S_{22} S_{22}^{-1}S_{21} = S_{12}S_{22}^{-1}S_{21} = M $$

Therefore, the matrix $M$ represents the component of the variance in the first set of variables that is explained by the second set of variables. The eigenvalue problem $(S_{12}S_{22}^{-1}S_{21})\mathbf{a} = \lambda S_{11}\mathbf{a}$ can be rewritten as $(S_{11}^{-1/2} S_{12}S_{22}^{-1}S_{21} S_{11}^{-1/2}) \mathbf{a}^* = \lambda \mathbf{a}^*$. This is finding the eigenvectors of the matrix $S_{11}^{-1}M$. This matrix represents the ratio of the explained variance to the total variance in the $X_1$ space. The eigenvectors $\mathbf{a}$ give the directions in the $X_1$ space that maximize this ratio, and the eigenvalues $\lambda$ are the squared canonical correlations, representing the proportion of variance of the canonical variate $U = \mathbf{a}^T X_1$ that is explained by the second set of variables.
