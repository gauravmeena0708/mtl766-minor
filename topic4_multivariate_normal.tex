\subsection*{Question 1}
\textbf{Question:} Write down the probability density function (pdf) of a $p$-variate normal distribution with mean vector $\boldsymbol{\mu}$ and covariance matrix $\Sigma$.

\textbf{Solution:}
The pdf for a random vector $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$ is:
$$ f(\mathbf{x}) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right) $$
This is valid for $\mathbf{x} \in \mathbb{R}^p$, and it requires that the covariance matrix $\Sigma$ be positive definite (and thus invertible).

\subsection*{Question 2}
\textbf{Question:} What are the main properties of the multivariate normal distribution? List at least three.

\textbf{Solution:}
1.  **Linear combinations are normal:** If $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$, then any linear combination $A\mathbf{X} + \mathbf{b}$ is also normally distributed.
2.  **Marginal distributions are normal:** All subsets of the components of $\mathbf{X}$ have multivariate normal distributions.
3.  **Zero covariance implies independence:** If two subsets of components of $\mathbf{X}$ have a zero covariance matrix, then they are statistically independent.
4.  **Conditional distributions are normal:** The conditional distribution of one subset of components, given the values of another subset, is also multivariate normal.

\subsection*{Question 3}
\textbf{Question:} Let $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$ and let $A$ be a $q \times p$ matrix of constants. Show that the linear combination $A\mathbf{X}$ is also multivariate normal. What are its mean and covariance matrix?

\textbf{Solution:}
The resulting distribution of $\mathbf{Y} = A\mathbf{X}$ is multivariate normal. We can find its mean and covariance as follows:
Mean:
$$ E(\mathbf{Y}) = E(A\mathbf{X}) = A E(\mathbf{X}) = A\boldsymbol{\mu} $$
Covariance:
$$ \text{Cov}(\mathbf{Y}) = \text{Cov}(A\mathbf{X}) = A \text{Cov}(\mathbf{X}) A^T = A \Sigma A^T $$
So, $\mathbf{Y} = A\mathbf{X} \sim N_q(A\boldsymbol{\mu}, A\Sigma A^T)$. A formal proof involves using moment generating functions or characteristic functions.

\subsection*{Question 4}
\textbf{Question:} Let $\mathbf{X} \sim N_2\left(\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 4 & 1 \\ 1 & 9 \end{pmatrix}\right)$. Let $A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$. Find the distribution of $A\mathbf{X}$.

\textbf{Solution:}
Let $\mathbf{Y} = A\mathbf{X}$. The distribution of $\mathbf{Y}$ is normal with mean $A\boldsymbol{\mu}$ and covariance $A\Sigma A^T$.
$$ A\boldsymbol{\mu} = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 3 \\ -1 \end{pmatrix} $$
$$ A\Sigma A^T = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 4 & 1 \\ 1 & 9 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}^T = \begin{pmatrix} 5 & 10 \\ 3 & -8 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 15 & -5 \\ -5 & 11 \end{pmatrix} $$
So, $A\mathbf{X} \sim N_2\left(\begin{pmatrix} 3 \\ -1 \end{pmatrix}, \begin{pmatrix} 15 & -5 \\ -5 & 11 \end{pmatrix}\right)$.

\subsection*{Question 5}
\textbf{Question:} Let $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$. If a subset of components of $\mathbf{X}$ has zero covariance with another subset, what does this imply about the independence of these subsets?

\textbf{Solution:}
For the multivariate normal distribution, zero covariance is a necessary and sufficient condition for independence. If we partition $\mathbf{X}$ into $\mathbf{X}_1$ and $\mathbf{X}_2$ and their cross-covariance $\Sigma_{12}$ is a zero matrix, then $\mathbf{X}_1$ and $\mathbf{X}_2$ are statistically independent.

\subsection*{Question 6}
\textbf{Question:} Let $\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix}$ be a partitioned multivariate normal random vector with corresponding partitioned mean $\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}$ and covariance $\Sigma = \begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}$. What is the marginal distribution of $\mathbf{X}_1$?

\textbf{Solution:}
One of the key properties of the MVN is that its marginal distributions are also normal. The marginal distribution of $\mathbf{X}_1$ is obtained by simply taking the corresponding blocks of the mean vector and covariance matrix.
$$ \mathbf{X}_1 \sim N_{p_1}(\boldsymbol{\mu}_1, \Sigma_{11}) $$
where $p_1$ is the dimension of $\mathbf{X}_1$.

\subsection*{Question 7}
\textbf{Question:} Given $\mathbf{X} \sim N_3\left(\begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}, \begin{pmatrix} 5 & 2 & 1 \\ 2 & 4 & -1 \\ 1 & -1 & 3 \end{pmatrix}\right)$, find the marginal distribution of $\begin{pmatrix} X_1 \\ X_3 \end{pmatrix}$.

\textbf{Solution:}
We select the components corresponding to $X_1$ and $X_3$ from the mean vector and covariance matrix.
Mean: $\boldsymbol{\mu}_{1,3} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$.
Covariance matrix: $\Sigma_{1,3} = \begin{pmatrix} 5 & 1 \\ 1 & 3 \end{pmatrix}$.
So, $\begin{pmatrix} X_1 \\ X_3 \end{pmatrix} \sim N_2\left(\begin{pmatrix} 1 \\ -1 \end{pmatrix}, \begin{pmatrix} 5 & 1 \\ 1 & 3 \end{pmatrix}\right)$.

\subsection*{Question 8}
\textbf{Question:} State the formula for the conditional distribution of $\mathbf{X}_1$ given $\mathbf{X}_2 = \mathbf{x}_2$ for a partitioned multivariate normal vector.

\textbf{Solution:}
The conditional distribution of $\mathbf{X}_1$ given $\mathbf{X}_2 = \mathbf{x}_2$ is also multivariate normal, with:
Mean: $E(\mathbf{X}_1 | \mathbf{X}_2=\mathbf{x}_2) = \boldsymbol{\mu}_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)$
Covariance: $\text{Cov}(\mathbf{X}_1 | \mathbf{X}_2=\mathbf{x}_2) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}$
Note that the conditional covariance does not depend on the value of $\mathbf{x}_2$.

\subsection*{Question 9}
\textbf{Question:} Let $\mathbf{X} \sim N_2\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 4 & 2 \\ 2 & 2 \end{pmatrix}\right)$. Find the conditional distribution of $X_1$ given $X_2 = 1$.

\textbf{Solution:}
Here, $\mathbf{X}_1=X_1$, $\mathbf{X}_2=X_2$, $\boldsymbol{\mu}_1=0, \boldsymbol{\mu}_2=0$, $\Sigma_{11}=4, \Sigma_{12}=2, \Sigma_{22}=2$.
Conditional Mean: $E(X_1 | X_2=1) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2) = 0 + 2 \cdot (1/2) \cdot (1-0) = 1$.
Conditional Variance: $\text{Var}(X_1 | X_2=1) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} = 4 - 2 \cdot (1/2) \cdot 2 = 4 - 2 = 2$.
So, $(X_1 | X_2=1) \sim N(1, 2)$.

\subsection*{Question 10}
\textbf{Question:} What is the distribution of the quadratic form $(\mathbf{X} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{X} - \boldsymbol{\mu})$ when $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$?

\textbf{Solution:}
The quadratic form $(\mathbf{X} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{X} - \boldsymbol{\mu})$ follows a chi-square distribution with $p$ degrees of freedom.
$$ (\mathbf{X} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{X} - \boldsymbol{\mu}) \sim \chi^2_p $$

\subsection*{Question 11}
\textbf{Question:} Explain how the result from Question 10 is used to construct a confidence ellipsoid for the population mean vector $\boldsymbol{\mu}$.

\textbf{Solution:}
From the central limit theorem, the sample mean $\bar{\mathbf{X}}$ is approximately $N_p(\boldsymbol{\mu}, \frac{1}{n}\Sigma)$.
Thus, $n(\bar{\mathbf{X}} - \boldsymbol{\mu})^T S^{-1} (\bar{\mathbf{X}} - \boldsymbol{\mu})$ is approximately $\chi^2_p$ for large $n$.
A $100(1-\alpha)\%$ confidence ellipsoid for $\boldsymbol{\mu}$ is the set of all $\boldsymbol{\mu}$ that satisfy:
$$ n(\bar{\mathbf{x}} - \boldsymbol{\mu})^T S^{-1} (\bar{\mathbf{x}} - \boldsymbol{\mu}) \le \chi^2_{p, \alpha} $$
where $\chi^2_{p, \alpha}$ is the upper $(100\alpha)$th percentile of the $\chi^2_p$ distribution.

\subsection*{Question 12}
\textbf{Question:} For a bivariate normal distribution ($p=2$), what is the equation for a 95% confidence ellipse for the mean vector $\boldsymbol{\mu}$? You can leave the answer in terms of the sample mean $\bar{\mathbf{x}}$, sample covariance $S$, and a chi-square critical value.

\textbf{Solution:}
The equation for a 95% confidence ellipse for $\boldsymbol{\mu}$ is given by the inequality:
$$ n(\bar{\mathbf{x}} - \boldsymbol{\mu})^T S^{-1} (\bar{\mathbf{x}} - \boldsymbol{\mu}) \le \chi^2_{2, 0.05} $$
where $n$ is the sample size, $\bar{\mathbf{x}}$ is the sample mean vector, $S$ is the sample covariance matrix, and $\chi^2_{2, 0.05} \approx 5.99$ is the critical value from a chi-square distribution with 2 degrees of freedom.

\subsection*{Question 13}
\textbf{Question:} Show that any linear combination of the components of a multivariate normal vector $\mathbf{X}$, say $\mathbf{a}^T\mathbf{X}$, follows a univariate normal distribution.

\textbf{Solution:}
This is a special case of the property in Question 3, where $A$ is a $1 \times p$ matrix (a row vector $\mathbf{a}^T$).
Let $Y = \mathbf{a}^T\mathbf{X}$. The mean of $Y$ is $E(Y) = E(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T E(\mathbf{X}) = \mathbf{a}^T\boldsymbol{\mu}$.
The variance of $Y$ is $\text{Var}(Y) = \text{Cov}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T \text{Cov}(\mathbf{X}) \mathbf{a} = \mathbf{a}^T\Sigma\mathbf{a}$.
Since $Y$ is a scalar, its distribution is univariate normal: $Y \sim N(\mathbf{a}^T\boldsymbol{\mu}, \mathbf{a}^T\Sigma\mathbf{a})$.

\subsection*{Question 14}
\textbf{Question:} If all marginal distributions of a random vector $\mathbf{X}$ are normal, is $\mathbf{X}$ necessarily multivariate normal? Explain.

\textbf{Solution:}
No. If $\mathbf{X}$ is multivariate normal, then all its marginals are normal. However, the converse is not true. It is possible to construct a joint distribution where the marginals are normal, but the joint distribution is not multivariate normal. A key feature of the MVN distribution is that the dependency structure is fully captured by the covariance matrix, which is not true for all distributions.

\subsection*{Question 15}
\textbf{Question:} Describe the shape and orientation of the contours of constant density for a multivariate normal distribution. What determines them?

\textbf{Solution:}
The contours of constant density for a multivariate normal distribution are ellipsoids.
- The center of the ellipsoids is the mean vector $\boldsymbol{\mu}$.
- The orientation of the ellipsoids is determined by the eigenvectors of the covariance matrix $\Sigma$. The eigenvectors are the principal axes of the ellipsoids.
- The lengths of the axes are determined by the eigenvalues of $\Sigma$. Larger eigenvalues correspond to longer axes, indicating greater variance in the direction of the corresponding eigenvector.

\subsection*{Question 16}
\textbf{Question:} Let $\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix}$ be partitioned as $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$, where $\Sigma = \begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}$. Derive the conditional distribution of $\mathbf{X}_1$ given $\mathbf{X}_2 = \mathbf{x}_2$.

\textbf{Solution:}
The derivation relies on finding a transformation of $\mathbf{X}$ that makes a component independent of $\mathbf{X}_2$. Consider the transformation:
$$ \mathbf{Y}_1 = \mathbf{X}_1 - \Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}_2 $$
$$ \mathbf{Y}_2 = \mathbf{X}_2 $$
This is a linear transformation of $\mathbf{X}$, so the joint distribution of $(\mathbf{Y}_1, \mathbf{Y}_2)$ is also multivariate normal. Let's find the covariance between $\mathbf{Y}_1$ and $\mathbf{Y}_2$:
$$ \text{Cov}(\mathbf{Y}_1, \mathbf{Y}_2) = \text{Cov}(\mathbf{X}_1 - \Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}_2, \mathbf{X}_2) $$
$$ = \text{Cov}(\mathbf{X}_1, \mathbf{X}_2) - \text{Cov}(\Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}_2, \mathbf{X}_2) $$
$$ = \Sigma_{12} - \Sigma_{12}\Sigma_{22}^{-1}\text{Cov}(\mathbf{X}_2, \mathbf{X}_2) = \Sigma_{12} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22} = \Sigma_{12} - \Sigma_{12} = \mathbf{0} $$
Since they are jointly normal and their covariance is zero, $\mathbf{Y}_1$ and $\mathbf{Y}_2$ are independent.
Therefore, the conditional distribution of $\mathbf{Y}_1$ given $\mathbf{Y}_2=\mathbf{y}_2$ (which is equivalent to $\mathbf{X}_2=\mathbf{x}_2$) is the same as its marginal distribution.
The marginal distribution of $\mathbf{Y}_1$ is normal with mean $E[\mathbf{Y}_1] = \boldsymbol{\mu}_1 - \Sigma_{12}\Sigma_{22}^{-1}\boldsymbol{\mu}_2$ and covariance $\text{Cov}(\mathbf{Y}_1) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}$.
So, $(\mathbf{Y}_1 | \mathbf{X}_2=\mathbf{x}_2) \sim N(\boldsymbol{\mu}_1 - \Sigma_{12}\Sigma_{22}^{-1}\boldsymbol{\mu}_2, \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$.
Now we substitute back $\mathbf{X}_1 - \Sigma_{12}\Sigma_{22}^{-1}\mathbf{x}_2$ for $\mathbf{Y}_1$.
The distribution of $(\mathbf{X}_1 | \mathbf{X}_2=\mathbf{x}_2)$ is normal with:
$$ E[\mathbf{X}_1 | \mathbf{X}_2=\mathbf{x}_2] = E[\mathbf{Y}_1 + \Sigma_{12}\Sigma_{22}^{-1}\mathbf{x}_2] = E[\mathbf{Y}_1] + \Sigma_{12}\Sigma_{22}^{-1}\mathbf{x}_2 = \boldsymbol{\mu}_1 - \Sigma_{12}\Sigma_{22}^{-1}\boldsymbol{\mu}_2 + \Sigma_{12}\Sigma_{22}^{-1}\mathbf{x}_2 = \boldsymbol{\mu}_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2) $$
$$ \text{Cov}(\mathbf{X}_1 | \mathbf{X}_2=\mathbf{x}_2) = \text{Cov}(\mathbf{Y}_1) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} $$
This gives the parameters of the conditional distribution.

\subsection*{Question 17}
\textbf{Question:} Let $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$. Use the moment generating function (MGF) to prove that for a constant matrix $A$ of size $q \times p$, the random vector $\mathbf{Y} = A\mathbf{X}$ is distributed as $N_q(A\boldsymbol{\mu}, A\Sigma A^T)$.

\textbf{Solution:}
The MGF of a random vector $\mathbf{X}$ is $M_{\mathbf{X}}(\mathbf{t}) = E[e^{\mathbf{t}^T\mathbf{X}}]$. For a $N_p(\boldsymbol{\mu}, \Sigma)$ distribution, the MGF is:
$$ M_{\mathbf{X}}(\mathbf{t}) = \exp(\mathbf{t}^T\boldsymbol{\mu} + \frac{1}{2}\mathbf{t}^T\Sigma\mathbf{t}) $$
Now, let's find the MGF of $\mathbf{Y} = A\mathbf{X}$. Let $\mathbf{s}$ be a $q \times 1$ vector.
$$ M_{\mathbf{Y}}(\mathbf{s}) = E[e^{\mathbf{s}^T\mathbf{Y}}] = E[e^{\mathbf{s}^T(A\mathbf{X})}] = E[e^{(\mathbf{s}^T A)\mathbf{X}}] $$
This has the same form as the MGF of $\mathbf{X}$, but with the vector $\mathbf{t}$ replaced by the vector $(\mathbf{s}^T A)^T = A^T\mathbf{s}$.
So, we can substitute $\mathbf{t} = A^T\mathbf{s}$ into the MGF of $\mathbf{X}$:
$$ M_{\mathbf{Y}}(\mathbf{s}) = M_{\mathbf{X}}(A^T\mathbf{s}) = \exp((A^T\mathbf{s})^T\boldsymbol{\mu} + \frac{1}{2}(A^T\mathbf{s})^T\Sigma(A^T\mathbf{s})) $$
$$ = \exp(\mathbf{s}^T A \boldsymbol{\mu} + \frac{1}{2}\mathbf{s}^T A \Sigma A^T \mathbf{s}) $$
This is the MGF of a multivariate normal distribution with mean vector $A\boldsymbol{\mu}$ and covariance matrix $A\Sigma A^T$.
By the uniqueness property of MGFs, we have proven that $\mathbf{Y} = A\mathbf{X} \sim N_q(A\boldsymbol{\mu}, A\Sigma A^T)$.

\subsection*{Question 18}
\textbf{Question:} Let $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$ and let $\Sigma = P \Lambda P^T$ be the spectral decomposition of $\Sigma$, where $P$ is the orthogonal matrix of eigenvectors and $\Lambda$ is the diagonal matrix of eigenvalues ($\lambda_1, \dots, \lambda_p$). Find the distribution of the transformed vector $\mathbf{Y} = P^T(\mathbf{X} - \boldsymbol{\mu})$.

\textbf{Solution:}
The vector $\mathbf{X} - \boldsymbol{\mu}$ is distributed as $N_p(\mathbf{0}, \Sigma)$.
We are applying a linear transformation $A = P^T$ to the vector $(\mathbf{X} - \boldsymbol{\mu})$.
Using the property of linear transformations of normal vectors, $\mathbf{Y}$ is also normally distributed.
The mean of $\mathbf{Y}$ is:
$$ E[\mathbf{Y}] = E[P^T(\mathbf{X} - \boldsymbol{\mu})] = P^T E[\mathbf{X} - \boldsymbol{\mu}] = P^T \mathbf{0} = \mathbf{0} $$
The covariance of $\mathbf{Y}$ is:
$$ \text{Cov}(\mathbf{Y}) = \text{Cov}(P^T(\mathbf{X} - \boldsymbol{\mu})) = P^T \text{Cov}(\mathbf{X} - \boldsymbol{\mu}) (P^T)^T $$
$$ = P^T \Sigma P = P^T (P \Lambda P^T) P $$
Since $P$ is an orthogonal matrix, $P^T P = I$.
$$ \text{Cov}(\mathbf{Y}) = (P^T P) \Lambda (P^T P) = I \Lambda I = \Lambda $$
So, $\mathbf{Y} = P^T(\mathbf{X} - \boldsymbol{\mu}) \sim N_p(\mathbf{0}, \Lambda)$.
This transformation is important as it transforms the correlated variables of $\mathbf{X}$ into a set of uncorrelated normal variables (the components of $\mathbf{Y}$). The variance of the $i$-th component of $\mathbf{Y}$ is the $i$-th eigenvalue $\lambda_i$. This is the basis of Principal Component Analysis.

\subsection*{Question 19}
\textbf{Question:} Let $W \sim W_p(n, I)$, i.e., a Wishart distribution with $n$ degrees of freedom and an identity scale matrix. The distribution of the largest eigenvalue of $W/n$, $\lambda_{max}$, is of great interest in random matrix theory (the Tracy-Widom distribution provides an approximation). Describe a Monte Carlo simulation procedure to estimate the 95th percentile of the distribution of $\lambda_{max}$.

\textbf{Solution:}
A direct analytical derivation of the exact distribution of the largest eigenvalue of a Wishart matrix is extremely complex. However, we can use a Monte Carlo simulation to estimate its percentiles. The procedure is as follows:

1.  **Set Parameters:** Define the parameters of the simulation: the dimension $p$, the degrees of freedom $n$, and the number of simulation replications, $N$ (e.g., $N=10,000$).

2.  **Generate Wishart Matrices:** In each replication $i$ from $1$ to $N$:
    a. Generate $n$ independent random vectors, $\mathbf{z}_1, \dots, \mathbf{z}_n$, where each $\mathbf{z}_j \sim N_p(\mathbf{0}, I)$.
    b. Construct the matrix $W_i = \sum_{j=1}^n \mathbf{z}_j \mathbf{z}_j^T$. This matrix $W_i$ is a random draw from the $W_p(n, I)$ distribution.

3.  **Calculate Largest Eigenvalue:** For each generated matrix $W_i$, compute its eigenvalues. Let $\lambda_{max, i}$ be the largest eigenvalue of the matrix $W_i/n$.

4.  **Collect Results:** Store the $N$ calculated largest eigenvalues: $\{\lambda_{max, 1}, \lambda_{max, 2}, \dots, \lambda_{max, N}\}$.

5.  **Estimate Percentile:** Sort the collected eigenvalues in ascending order. The 95th percentile is estimated by finding the value at the $0.95 \times N$ position in the sorted list. For example, if $N=10,000$, the 95th percentile estimate would be the 9500th value in the sorted list.

This simulation approach allows us to approximate the sampling distribution of the largest eigenvalue and its properties without needing to solve the complex analytical theory.

\subsection*{Question 20}
\textbf{Question:} Let $\mathbf{X} \sim N_p(\boldsymbol{\mu}, \Sigma)$, and let $\mathbf{a}$ and $\mathbf{b}$ be two constant $p \times 1$ vectors. Find the covariance of the two linear combinations $Y_1 = \mathbf{a}^T\mathbf{X}$ and $Y_2 = \mathbf{b}^T\mathbf{X}$. What is the condition on $\mathbf{a}$, $\mathbf{b}$ and $\Sigma$ for $Y_1$ and $Y_2$ to be independent?

\textbf{Solution:}
First, we find the covariance between $Y_1$ and $Y_2$.
$$ \text{Cov}(Y_1, Y_2) = \text{Cov}(\mathbf{a}^T\mathbf{X}, \mathbf{b}^T\mathbf{X}) $$
Using the formula for the covariance of two linear combinations, $\text{Cov}(A\mathbf{X}, B\mathbf{X}) = A\Sigma B^T$:
Here, $A = \mathbf{a}^T$ and $B = \mathbf{b}^T$. So,
$$ \text{Cov}(Y_1, Y_2) = \mathbf{a}^T \Sigma (\mathbf{b}^T)^T = \mathbf{a}^T \Sigma \mathbf{b} $$
Since the result is a scalar, this is the covariance.

For $Y_1$ and $Y_2$ to be independent, their covariance must be zero. This is because they are both linear combinations of a multivariate normal vector, which means they are jointly normally distributed. For a bivariate normal distribution, zero covariance implies independence.
The condition for independence is therefore:
$$ \mathbf{a}^T \Sigma \mathbf{b} = 0 $$
Geometrically, this means that the vectors $\mathbf{a}$ and $\mathbf{b}$ must be orthogonal with respect to an inner product defined by the covariance matrix $\Sigma$. This is sometimes called $\Sigma$-orthogonality or conjugate orthogonality. If $\Sigma=I$ (i.e., the components of $\mathbf{X}$ are uncorrelated with unit variance), this condition reduces to the standard orthogonality condition $\mathbf{a}^T\mathbf{b}=0$.
